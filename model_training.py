# ===========================
# model_training.py
# Script pour entraÃ®ner le modÃ¨le de prÃ©diction de dÃ©pression
# ===========================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import joblib
import os

# ---------------------------
# 1ï¸âƒ£ Charger les donnÃ©es
# ---------------------------
print("ğŸ“‚ Chargement des donnÃ©es...")
data = pd.read_csv("data/mental_health_lifestyle.csv")
print(f"âœ… Dataset chargÃ© : {data.shape[0]} lignes, {data.shape[1]} colonnes\n")

# ---------------------------
# 2ï¸âƒ£ Exploration rapide
# ---------------------------
print("ğŸ“Š AperÃ§u des donnÃ©es :")
print(data.head())
print("\nğŸ“ˆ Informations sur le dataset :")
print(data.info())
print("\nğŸ” Valeurs manquantes :")
print(data.isnull().sum())

# Supprimer les valeurs manquantes
data = data.dropna()
print(f"\nâœ… AprÃ¨s nettoyage : {data.shape[0]} lignes\n")

# ---------------------------
# 3ï¸âƒ£ PrÃ©paration des donnÃ©es
# ---------------------------
print("âš™ï¸ PrÃ©paration des features et target...")

# Identifier la colonne cible (ajuster selon votre dataset)
target_col = 'Depression'  # ou 'depression', 'depressed', etc.

# SÃ©parer features et target
X = data.drop(columns=[target_col])
y = data[target_col]

# Encoder les variables catÃ©gorielles si nÃ©cessaire
categorical_cols = X.select_dtypes(include=['object']).columns
if len(categorical_cols) > 0:
    print(f"ğŸ”¤ Encodage des variables catÃ©gorielles : {list(categorical_cols)}")
    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

print(f"âœ… Features : {X.shape[1]} colonnes")
print(f"âœ… Target distribution :\n{y.value_counts()}\n")

# ---------------------------
# 4ï¸âƒ£ Division train/test
# ---------------------------
print("ğŸ”€ Division train/test (80/20)...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"âœ… Train set : {X_train.shape[0]} Ã©chantillons")
print(f"âœ… Test set : {X_test.shape[0]} Ã©chantillons\n")

# ---------------------------
# 5ï¸âƒ£ Normalisation
# ---------------------------
print("ğŸ“ Normalisation des donnÃ©es...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("âœ… Normalisation terminÃ©e\n")

# ---------------------------
# 6ï¸âƒ£ EntraÃ®nement du modÃ¨le
# ---------------------------
print("ğŸ¤– EntraÃ®nement du modÃ¨le Random Forest...")
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train_scaled, y_train)
print("âœ… ModÃ¨le entraÃ®nÃ©\n")

# ---------------------------
# 7ï¸âƒ£ Validation croisÃ©e
# ---------------------------
print("ğŸ”„ Validation croisÃ©e (5-fold)...")
cv_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=5, scoring='accuracy')
print(f"âœ… CV Scores : {cv_scores}")
print(f"âœ… Moyenne : {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\n")

# ---------------------------
# 8ï¸âƒ£ Ã‰valuation sur le test set
# ---------------------------
print("ğŸ“Š Ã‰valuation sur le test set...")
y_pred = rf_model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"\nğŸ¯ Accuracy : {accuracy:.4f}")
print("\nğŸ“‹ Rapport de classification :")
print(classification_report(y_test, y_pred))
print("\nğŸ”¢ Matrice de confusion :")
print(confusion_matrix(y_test, y_pred))

# ---------------------------
# 9ï¸âƒ£ Importance des features
# ---------------------------
print("\nğŸ“Œ Top 10 features les plus importantes :")
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)
print(feature_importance.head(10))

# ---------------------------
# ğŸ”Ÿ Optimisation des hyperparamÃ¨tres (optionnel)
# ---------------------------
print("\nğŸ”§ Optimisation des hyperparamÃ¨tres (GridSearch)...")
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_scaled, y_train)
print(f"\nâœ… Meilleurs paramÃ¨tres : {grid_search.best_params_}")
print(f"âœ… Meilleur score CV : {grid_search.best_score_:.4f}")

# Utiliser le meilleur modÃ¨le
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test_scaled)
accuracy_best = accuracy_score(y_test, y_pred_best)
print(f"âœ… Accuracy avec meilleurs paramÃ¨tres : {accuracy_best:.4f}\n")

# ---------------------------
# 1ï¸âƒ£1ï¸âƒ£ Sauvegarder le modÃ¨le et le scaler
# ---------------------------
print("ğŸ’¾ Sauvegarde du modÃ¨le et du scaler...")
os.makedirs('models', exist_ok=True)

joblib.dump(best_model, 'models/depression_model.pkl')
joblib.dump(scaler, 'models/scaler.pkl')

# Sauvegarder aussi les noms des features pour l'application
joblib.dump(X.columns.tolist(), 'models/feature_names.pkl')

print("âœ… ModÃ¨le sauvegardÃ© : models/depression_model.pkl")
print("âœ… Scaler sauvegardÃ© : models/scaler.pkl")
print("âœ… Feature names sauvegardÃ©s : models/feature_names.pkl")

print("\nğŸ‰ EntraÃ®nement terminÃ© avec succÃ¨s !")